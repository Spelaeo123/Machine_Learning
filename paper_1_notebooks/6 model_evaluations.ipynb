{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load variables stored by data_preproccessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_formodel\n",
    "%store -r test_data\n",
    "%store -r my_data\n",
    "%store -r uniques\n",
    "%store -r best_feats\n",
    "%store -r X_test_labeled_df\n",
    "%store -r site_frequencies_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* save_plots -> True|False\n",
    "* random_seed_state -> number, sets random state for model and for stratified splits \n",
    "* classify_bedrock_only -> True|False\n",
    "* pickle_model -> True|False, wether model should be serialised and saved\n",
    "* pickle_model_name -> string, name of serialised model\n",
    "* grid_search -> True|False, if set to true then grid search is performed to identify optimum hyperparamaters for model \n",
    "* scale -> True|False if set to True then features scaled to all have mean value 0 and standard deviation 1\n",
    "* pickle_file_path -> string,  filepath for serialised model to be saved to\n",
    "* modelName -> string, type of model to use 'rfc'|'gbm'|'svm'|'knn'\n",
    "* trainTestSplitTotalIters -> total number of iterations of train test split for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = True\n",
    "random_seed_state = 42\n",
    "classify_bedrock_only = False\n",
    "grid_search = False\n",
    "scale = True\n",
    "save_predictions = False\n",
    "modelName = 'gbm'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if only bedrock sites are classified then classes are label encoded, if bedrock sites alone are not being classified then the class sites would have already been label encoded in the 1 data_preproccessing notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if classify_bedrock_only:\n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n",
    "    train_data_formodel = train_data_formodel[train_data_formodel['Geology']=='Bedrock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts of instances in all classes before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     105\n",
       "17    100\n",
       "18     61\n",
       "0      53\n",
       "10     47\n",
       "13     45\n",
       "15     36\n",
       "16     36\n",
       "2      36\n",
       "12     30\n",
       "11     30\n",
       "8      30\n",
       "7      30\n",
       "5      30\n",
       "6      27\n",
       "9      27\n",
       "1      24\n",
       "14     21\n",
       "3      18\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_formodel['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The class column is stored as the variable y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_data_formodel['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The variables identified as best by the 2 feature_selection notebook are used as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feats = train_data_formodel[best_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### address class imbalance using synthetic minority oversampling technique (SMOTE) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale:\n",
    "    my_scaler = StandardScaler()\n",
    "    X = np.array(my_scaler.fit_transform(np.array(train_data_feats)))\n",
    "else:\n",
    "    X = np.array(np.array(train_data_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dimensions of the class and features are checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786, 25)\n",
      "(786,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=random_seed_state)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carry out 10-f0ld stratified cross validation, class f1 scores and macro f1 scores are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelName == 'rfc':\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=random_seed_state)\n",
    "\n",
    "    class_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    feat_imp =[]\n",
    "    f1_dict = {}\n",
    "    feat_imp_dict = {}\n",
    "    count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index] \n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify = y)\n",
    "        count = count + 1\n",
    "        print('making model:')\n",
    "        key = 'round' + str(count)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "        X_post_smote, y_post_smote = SMOTE(random_state=42).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "        ###this section optimises model paramaters by gridsearch \n",
    "\n",
    "        esti = RandomForestClassifier(max_features = 'auto', random_state = random_seed_state)\n",
    "\n",
    "        n_estimators = [2000, 2500, 3000]\n",
    "        max_depth = [80, 100, 120]\n",
    "        min_samples_split = [2, 3, 4]\n",
    "        min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "        param_grid = {\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'n_estimators':n_estimators \n",
    "                      }\n",
    "\n",
    "        clf = GridSearchCV(estimator = esti, param_grid= param_grid,\n",
    "                                  n_jobs=-1, scoring='f1_macro', cv = 5, verbose=3)\n",
    "        print('running grid search on this training data fold')\n",
    "        clf.fit(X_post_smote, y_post_smote)\n",
    "        optimumEstimator = clf.best_estimator_\n",
    "        optimumEstimator.fit(X_post_smote, y_post_smote)\n",
    "        print('gridsearch identified optimum paramaters for the current training data fold, now model with optimumn paramaters predicting using test_data for evaluation')\n",
    "\n",
    "        y_pred = optimumEstimator.predict(X_test)\n",
    "        class_f1_scores = f1_score(y_test, y_pred, average = None)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        macro_f1_scores.append(f1_score(y_test, y_pred, average = 'macro'))\n",
    "        f1_dict[key] = class_f1_scores \n",
    "        feat_imp_dict[key] = optimumEstimator.feature_importances_\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model:\n",
      "1\n",
      "running grid search on this training data fold\n",
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "if modelName == 'gbm':\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=random_seed_state)\n",
    "\n",
    "    class_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    feat_imp =[]\n",
    "    f1_dict = {}\n",
    "    feat_imp_dict = {}\n",
    "    count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index] \n",
    "        count = count + 1\n",
    "        print('making model:')\n",
    "        key = 'round' + str(count)\n",
    "        print(count)\n",
    "\n",
    "\n",
    "        X_post_smote, y_post_smote = SMOTE(random_state=42).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "        ###this section optimises model paramaters by gridsearch \n",
    "\n",
    "        esti = xgb.XGBClassifier()\n",
    "\n",
    "        n_estimators = [2000, 2500, 3000]        \n",
    "        max_depth = [80, 100, 120]\n",
    "        min_child_weight = [0,1,3]\n",
    "        gamma = [0,3,5,7,9]\n",
    "\n",
    "\n",
    "\n",
    "        param_grid = {\n",
    "                    'n_estimators':n_estimators,\n",
    "                   'max_depth': max_depth,\n",
    "                       'min_child_weight':min_child_weight,\n",
    "                       'gamma':gamma\n",
    "                      }\n",
    "\n",
    "        clf = GridSearchCV(estimator = esti, param_grid= param_grid,\n",
    "                                  n_jobs=-1, scoring='f1_macro', cv = 5, verbose=3)\n",
    "        print('running grid search on this training data fold')\n",
    "        clf.fit(X_post_smote, y_post_smote)\n",
    "        optimumEstimator = clf.best_estimator_\n",
    "        optimumEstimator.fit(X_post_smote, y_post_smote)\n",
    "        print('gridsearch identified optimum paramaters for the current training data fold, now model with optimumn paramaters predicting using test_data for evaluation')\n",
    "\n",
    "        y_pred = optimumEstimator.predict(X_test)\n",
    "        class_f1_scores = f1_score(y_test, y_pred, average = None)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        macro_f1_scores.append(f1_score(y_test, y_pred, average = 'macro'))\n",
    "        f1_dict[key] = class_f1_scores \n",
    "        feat_imp_dict[key] = optimumEstimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelName == 'svm':\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=random_seed_state)\n",
    "\n",
    "    class_f1_scores = []\n",
    "    macro_f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    feat_imp =[]\n",
    "    f1_dict = {}\n",
    "    feat_imp_dict = {}\n",
    "    count = 0\n",
    "\n",
    "  \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index] \n",
    "        \n",
    "        count = count + 1\n",
    "        print('making model:')\n",
    "        key = 'round' + str(count)\n",
    "        print(count)\n",
    "        X_post_smote, y_post_smote = SMOTE(random_state=42).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "        ###this section optimises model paramaters by gridsearch \n",
    "\n",
    "        esti = SVC()\n",
    "\n",
    "\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100], \n",
    "            'class_weight':['balanced', None], \n",
    "            'gamma':[0.001, 0.01, 0.1, 1, 10]\n",
    "                }\n",
    "\n",
    "        clf = GridSearchCV(estimator = esti, param_grid= param_grid,\n",
    "                                  n_jobs=-1, scoring='f1_macro', cv = 5, verbose=3)\n",
    "        print('running grid search on this training data fold')\n",
    "        clf.fit(X_post_smote, y_post_smote)\n",
    "        optimumEstimator = clf.best_estimator_\n",
    "        optimumEstimator.fit(X_post_smote, y_post_smote)\n",
    "        print('gridsearch identified optimum paramaters for the current training data fold, now model with optimumn paramaters predicting using test_data for evaluation')\n",
    "\n",
    "        y_pred = optimumEstimator.predict(X_test)\n",
    "        class_f1_scores = f1_score(y_test, y_pred, average = None)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        macro_f1_scores.append(f1_score(y_test, y_pred, average = 'macro'))\n",
    "        f1_dict[key] = class_f1_scores \n",
    "        #feat_imp_dict[key] = optimumEstimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a95fac89fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "f1_df = pd.DataFrame(data = f1_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1_dict:\n",
    "    print(len(f1_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the encodings for the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_formodel['class'].unique())\n",
    "print(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final = pd.concat([f1_df, pd.Series(uniques)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final.rename(columns={0:'class'}, inplace=True)\n",
    "f1_df_final.set_index('class', drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the distribution of class f1 scores from 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(data = f1_df_final.T)\n",
    "plot.set_title('F1 scores for each site', fontdict={'fontsize': 14})\n",
    "plot.set_ylabel('F1 score', fontdict={'fontsize': 11})\n",
    "plot.set_xlabel(\"Archeological site\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('figures/site_specific_f1_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_plots:\n",
    "    pd.DataFrame(data = f1_df_final.T.median()).to_csv('figures/median_class_f1_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the macro F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40ad8bc5430d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8.27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmacro_f1_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(macro_f1_scores)\n",
    "plot.set_title('F1 score', fontdict={'fontsize': 14})\n",
    "plot.set_xlabel(\"F1-score\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('figures/average-weighted f1_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_plots:\n",
    "    pd.Series(pd.Series(macro_f1_scores).median()).to_csv('figures/median_macro_f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(macro_f1_scores).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df = pd.DataFrame(data = feat_imp_dict)\n",
    "feat_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final = pd.concat([feat_imp_df, pd.Series(my_data[best_feats].columns.values)], axis = 1)\n",
    "feat_imp_df_final.rename(columns = {0:'element'}, inplace = True )\n",
    "feat_imp_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final.set_index('element', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final_plot = feat_imp_df_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final_plot\n",
    "\n",
    "elements = feat_imp_df_final_plot.columns.values \n",
    "mean_feature_importance = []\n",
    "for col in list(feat_imp_df_final_plot.columns.values):\n",
    "    mean_feature_importance.append(feat_imp_df_final_plot[col].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df = pd.concat([pd.Series(elements), pd.Series(mean_feature_importance)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df.rename(columns={0:'elements', 1:'mean_importance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df.sort_values(by='mean_importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_col_names = list(mean_feature_importance_df['elements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style()\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "plot = sns.boxplot(data = feat_imp_df_final_plot[ordered_col_names])\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=90, ha = 'left')\n",
    "plot.set_title('Feature (element) importance', fontdict={'fontsize': 20})\n",
    "plot.set_ylabel('Feature importance', fontdict={'fontsize': 15})\n",
    "plot.set_xlabel(\"Element\", fontdict={'fontsize': 15})\n",
    "\n",
    "if save_plots:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('figures/feature_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show relationship between sample size and class F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_frequencies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_frequencies_df.rename(columns = {'Site':'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_for_plot = f1_df_final.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_for_plot['Mean F1 Score'] = f1_scores_for_plot.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = site_frequencies_df.set_index('class').join(other = f1_scores_for_plot.set_index('class'), how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forPlot = combined_df.reset_index(drop = False).rename(columns = {'index':'class'})[['class', 'Number of Observations', 'Mean F1 Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8,7)})\n",
    "plot = sns.scatterplot(x ='Number of Observations', y = 'Mean F1 Score', data = forPlot, ci=False)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Mean F1 score vs number of observations for each class\")\n",
    "fig = plot.get_figure()\n",
    "if save_plots:\n",
    "    fig.savefig('figures/f1scoresvsnumberobservations{0}.png'.format(modelName))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_for_plot.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
