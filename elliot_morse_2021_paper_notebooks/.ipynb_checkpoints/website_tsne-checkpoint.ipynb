{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* data_input_path -> string, filepath to data to be read in\n",
    "\n",
    "* grouping -> boolean, if set to True them many sites are grouped\n",
    "* reduced_grouping -> boolean, if set to True then less sites grouped, only bedrock sites 'WB' and 'BX are grouped into one class and superficial sites 'SV' and 'SE' are grouped into one class.\n",
    "* raw -> boolean, if set to True then no grouping done\n",
    "* drop_semi_bedrock ->  True|False, if set to True then some bedrock sites deemed to be semi-bedrock sites are not used for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = '../data/raw_data.csv'\n",
    "\n",
    "drop_semi_bedrock = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_test_labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bedrock        808\n",
       "Superficial    435\n",
       "Artefacts      363\n",
       "Name: Geology, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.Geology.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw sample names including sample sites and artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions for making target classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### targets for classification are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_semi_bedrock:\n",
    "    my_data = my_data[(my_data['class'] != 'BM') & (my_data['class'] != 'BC') & (my_data['class'] != 'BP') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data[my_data['Site']!='BP']\n",
    "my_data = my_data[my_data['Site']!='BX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove '<' signs and commas from feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22856.10it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22353.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21714.85it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21508.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20988.21it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 19877.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21340.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21904.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20521.12it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20884.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20415.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21661.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21753.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21737.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21711.08it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21875.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21536.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20229.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21931.11it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22031.10it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21835.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21001.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21400.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 19381.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21720.61it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21491.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 19828.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21573.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21746.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21808.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21208.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21913.50it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21645.27it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22052.03it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21379.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21658.49it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21360.49it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20747.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21083.59it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21940.61it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21417.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21360.27it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20686.03it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22066.59it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21433.78it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20968.70it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20915.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21606.15it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22680.69it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 22028.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20844.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 21331.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1522/1522 [00:00<00:00, 20695.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove rows where there are all element abundances are na values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data.dropna(subset=my_data.columns.values[9:-1], how = 'all' , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers defined as any values that exceed 2 standard deviations from the mean, such values are changed to the mean for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23960.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23275.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24399.76it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24445.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24034.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23246.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23382.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23056.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23524.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23690.71it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22985.49it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 20504.56it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22223.87it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22096.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24278.70it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24820.34it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23785.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23895.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23542.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23198.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24766.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23543.81it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24580.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24681.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23265.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24604.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24502.05it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23833.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24804.39it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24203.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23322.37it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22926.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24885.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24281.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22745.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24506.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22795.21it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24652.34it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24994.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22489.69it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24716.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22755.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24338.70it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 25223.48it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 23388.31it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24517.61it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24561.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 22394.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 20871.50it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24641.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24258.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 21322.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1491/1491 [00:00<00:00, 24964.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into 'train_data' and 'test_data', the former consists of samples from known geological sites and the latter from flint artefacts fow which the original geological source site is unknown and to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analysis     False\n",
       "Geology      False\n",
       "Province      True\n",
       "Region        True\n",
       "Site         False\n",
       "SubSite       True\n",
       "Formation     True\n",
       "Band          True\n",
       "Nodule       False\n",
       "Li7          False\n",
       "Be9          False\n",
       "B11          False\n",
       "Mg24         False\n",
       "Al27         False\n",
       "Si28         False\n",
       "P31          False\n",
       "S33          False\n",
       "K39          False\n",
       "Ca42         False\n",
       "Sc45         False\n",
       "Ti47         False\n",
       "V51          False\n",
       "Cr52         False\n",
       "Mn55         False\n",
       "Fe56         False\n",
       "Co59         False\n",
       "Ni60         False\n",
       "Cu63         False\n",
       "Zn68         False\n",
       "Ga69         False\n",
       "Ge72         False\n",
       "As75         False\n",
       "Rb85         False\n",
       "Sr88         False\n",
       "Y89          False\n",
       "Zr90         False\n",
       "Nb93         False\n",
       "Mo95         False\n",
       "Cd111        False\n",
       "In115        False\n",
       "Sn118        False\n",
       "Cs133        False\n",
       "Ba137        False\n",
       "La139        False\n",
       "Ce140        False\n",
       "Pr141        False\n",
       "Nd146        False\n",
       "Sm147        False\n",
       "Eu153        False\n",
       "Gd157        False\n",
       "Tb159        False\n",
       "Dy163        False\n",
       "Ho165        False\n",
       "Er166        False\n",
       "Tm169        False\n",
       "Yb172        False\n",
       "Lu175        False\n",
       "Hf178        False\n",
       "Ta181        False\n",
       "Pb208        False\n",
       "Th232        False\n",
       "U238         False\n",
       "class         True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'training_data' is split into two datasets, one consisting of samples from superficial sites and one containing samples from bedrock sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### four datasets are created, one containing all train data (bedrock and superficial types), one containing just bedrock types, one containing just superficial types and one containing the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]\n",
    "element_data_everything = my_data[my_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491, 53)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_data_everything.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1128, 53)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Li7</th>\n",
       "      <th>Be9</th>\n",
       "      <th>B11</th>\n",
       "      <th>Mg24</th>\n",
       "      <th>Al27</th>\n",
       "      <th>Si28</th>\n",
       "      <th>P31</th>\n",
       "      <th>S33</th>\n",
       "      <th>K39</th>\n",
       "      <th>Ca42</th>\n",
       "      <th>Sc45</th>\n",
       "      <th>Ti47</th>\n",
       "      <th>V51</th>\n",
       "      <th>Cr52</th>\n",
       "      <th>Mn55</th>\n",
       "      <th>Fe56</th>\n",
       "      <th>Co59</th>\n",
       "      <th>Ni60</th>\n",
       "      <th>Cu63</th>\n",
       "      <th>Zn68</th>\n",
       "      <th>Ga69</th>\n",
       "      <th>Ge72</th>\n",
       "      <th>As75</th>\n",
       "      <th>Rb85</th>\n",
       "      <th>Sr88</th>\n",
       "      <th>Y89</th>\n",
       "      <th>Zr90</th>\n",
       "      <th>Nb93</th>\n",
       "      <th>Mo95</th>\n",
       "      <th>Cd111</th>\n",
       "      <th>In115</th>\n",
       "      <th>Sn118</th>\n",
       "      <th>Cs133</th>\n",
       "      <th>Ba137</th>\n",
       "      <th>La139</th>\n",
       "      <th>Ce140</th>\n",
       "      <th>Pr141</th>\n",
       "      <th>Nd146</th>\n",
       "      <th>Sm147</th>\n",
       "      <th>Eu153</th>\n",
       "      <th>Gd157</th>\n",
       "      <th>Tb159</th>\n",
       "      <th>Dy163</th>\n",
       "      <th>Ho165</th>\n",
       "      <th>Er166</th>\n",
       "      <th>Tm169</th>\n",
       "      <th>Yb172</th>\n",
       "      <th>Lu175</th>\n",
       "      <th>Hf178</th>\n",
       "      <th>Ta181</th>\n",
       "      <th>Pb208</th>\n",
       "      <th>Th232</th>\n",
       "      <th>U238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.63</td>\n",
       "      <td>0.12</td>\n",
       "      <td>48.36</td>\n",
       "      <td>154.630000</td>\n",
       "      <td>943.71</td>\n",
       "      <td>464944.180000</td>\n",
       "      <td>50.280000</td>\n",
       "      <td>538.57</td>\n",
       "      <td>455.94</td>\n",
       "      <td>712.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>15.58</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.62</td>\n",
       "      <td>10.82</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.43</td>\n",
       "      <td>12.94</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.54</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.50</td>\n",
       "      <td>0.09</td>\n",
       "      <td>44.77</td>\n",
       "      <td>33.872347</td>\n",
       "      <td>1077.11</td>\n",
       "      <td>465010.940000</td>\n",
       "      <td>70.910000</td>\n",
       "      <td>438.20</td>\n",
       "      <td>387.82</td>\n",
       "      <td>515.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>18.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.01</td>\n",
       "      <td>11.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>8.93</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.45</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8.04</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>44.88</td>\n",
       "      <td>42.700000</td>\n",
       "      <td>620.21</td>\n",
       "      <td>465295.410000</td>\n",
       "      <td>104.470000</td>\n",
       "      <td>372.66</td>\n",
       "      <td>363.71</td>\n",
       "      <td>957.89</td>\n",
       "      <td>0.76</td>\n",
       "      <td>19.89</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.21</td>\n",
       "      <td>87.99</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.53</td>\n",
       "      <td>11.98</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>8.52</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.16</td>\n",
       "      <td>0.73</td>\n",
       "      <td>47.06</td>\n",
       "      <td>33.872347</td>\n",
       "      <td>1143.19</td>\n",
       "      <td>462172.810919</td>\n",
       "      <td>2420.945164</td>\n",
       "      <td>1075.89</td>\n",
       "      <td>547.55</td>\n",
       "      <td>2174.30</td>\n",
       "      <td>0.43</td>\n",
       "      <td>42.30</td>\n",
       "      <td>0.67</td>\n",
       "      <td>152.42</td>\n",
       "      <td>4.84</td>\n",
       "      <td>145.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.45</td>\n",
       "      <td>5.02</td>\n",
       "      <td>17.15</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.76</td>\n",
       "      <td>13.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8.74</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.71</td>\n",
       "      <td>0.32</td>\n",
       "      <td>48.26</td>\n",
       "      <td>33.520000</td>\n",
       "      <td>547.22</td>\n",
       "      <td>465027.110000</td>\n",
       "      <td>44.440000</td>\n",
       "      <td>464.78</td>\n",
       "      <td>278.25</td>\n",
       "      <td>1551.63</td>\n",
       "      <td>0.71</td>\n",
       "      <td>11.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>25.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.80</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.28</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Li7   Be9    B11        Mg24     Al27           Si28          P31  \\\n",
       "0  15.63  0.12  48.36  154.630000   943.71  464944.180000    50.280000   \n",
       "1  11.50  0.09  44.77   33.872347  1077.11  465010.940000    70.910000   \n",
       "2  20.05  0.06  44.88   42.700000   620.21  465295.410000   104.470000   \n",
       "3  11.16  0.73  47.06   33.872347  1143.19  462172.810919  2420.945164   \n",
       "4  17.71  0.32  48.26   33.520000   547.22  465027.110000    44.440000   \n",
       "\n",
       "       S33     K39     Ca42  Sc45   Ti47   V51    Cr52  Mn55    Fe56  Co59  \\\n",
       "0   538.57  455.94   712.39  0.42  15.58  0.27    3.30  0.69    8.46  0.05   \n",
       "1   438.20  387.82   515.24  0.44  18.47  0.29    3.45  1.01   11.59  0.11   \n",
       "2   372.66  363.71   957.89  0.76  19.89  0.55    3.25  1.21   87.99  0.21   \n",
       "3  1075.89  547.55  2174.30  0.43  42.30  0.67  152.42  4.84  145.34  0.30   \n",
       "4   464.78  278.25  1551.63  0.71  11.18  0.27    2.56  1.73   25.38  0.05   \n",
       "\n",
       "   Ni60  Cu63   Zn68  Ga69  Ge72  As75  Rb85   Sr88   Y89  Zr90  Nb93  Mo95  \\\n",
       "0  0.80  1.62  10.82  0.25  1.22  0.16  0.43  12.94  0.88  1.51  0.09  0.05   \n",
       "1  0.36  0.53   8.93  0.34  0.85  0.10  0.45  13.22  0.95  1.74  0.07  0.01   \n",
       "2  1.68  1.53  11.98  0.25  1.71  0.13  0.43   8.52  0.87  0.93  0.10  0.02   \n",
       "3  2.45  5.02  17.15  0.35  2.13  0.84  0.76  13.16  0.97  2.00  0.10  0.29   \n",
       "4  0.80  0.55   9.80  0.41  1.41  0.12  0.28   9.90  0.90  0.90  0.08  0.04   \n",
       "\n",
       "   Cd111  In115  Sn118  Cs133  Ba137  La139  Ce140  Pr141  Nd146  Sm147  \\\n",
       "0   0.02   0.00   0.05   0.01   6.54   0.84   0.95   0.23   0.87   0.16   \n",
       "1   0.02   0.00   0.04   0.02   8.04   0.92   1.01   0.23   0.98   0.18   \n",
       "2   0.02   0.00   0.05   0.01   3.13   0.90   1.08   0.26   0.84   0.15   \n",
       "3   0.18   0.01   0.78   0.04   8.74   0.93   0.95   0.21   0.75   0.13   \n",
       "4   0.10   0.00   0.09   0.01   2.74   0.97   1.09   0.27   1.00   0.17   \n",
       "\n",
       "   Eu153  Gd157  Tb159  Dy163  Ho165  Er166  Tm169  Yb172  Lu175  Hf178  \\\n",
       "0   0.04   0.16   0.02   0.11   0.03   0.06   0.01   0.02   0.00   0.04   \n",
       "1   0.04   0.18   0.02   0.13   0.03   0.06   0.01   0.04   0.01   0.05   \n",
       "2   0.04   0.19   0.02   0.14   0.02   0.07   0.01   0.06   0.00   0.02   \n",
       "3   0.04   0.25   0.02   0.09   0.03   0.05   0.00   0.03   0.00   0.08   \n",
       "4   0.04   0.19   0.02   0.15   0.03   0.05   0.01   0.05   0.01   0.02   \n",
       "\n",
       "   Ta181  Pb208  Th232  U238  \n",
       "0   0.01   0.24   0.07  0.05  \n",
       "1   0.00   0.07   0.08  0.04  \n",
       "2   0.01   0.46   0.05  0.05  \n",
       "3   0.00   0.64   0.05  0.03  \n",
       "4   0.01   0.59   0.06  0.09  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_data_everything.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features are standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()\n",
    "my_scaler_everything = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)\n",
    "element_data_everything_scaled = my_scaler_everything.fit_transform(element_data_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The four datasets are transformed using Principal component analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "my_pca_everything = PCA(n_components=element_data_everything_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)\n",
    "element_data_everything_pca = my_pca_everything.fit_transform(element_data_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print details of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(copy=True, iterated_power='auto', n_components=53, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)\n"
     ]
    }
   ],
   "source": [
    "print(my_pca_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the principal components for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)\n",
    "PC_df_everything = pd.DataFrame(data = element_data_everything_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Distributed Stochastic Neighbour Embedding is done on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1128 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 1128 samples in 0.183s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1128\n",
      "[t-SNE] Computed conditional probabilities for sample 1128 / 1128\n",
      "[t-SNE] Mean sigma: 2.025902\n",
      "[t-SNE] Computed conditional probabilities in 0.064s\n",
      "[t-SNE] Iteration 50: error = 71.7872467, gradient norm = 0.1449712 (50 iterations in 4.767s)\n",
      "[t-SNE] Iteration 100: error = 73.1344299, gradient norm = 0.1326146 (50 iterations in 4.345s)\n",
      "[t-SNE] Iteration 150: error = 73.2917786, gradient norm = 0.1184866 (50 iterations in 3.046s)\n",
      "[t-SNE] Iteration 200: error = 74.2052460, gradient norm = 0.0961905 (50 iterations in 2.948s)\n",
      "[t-SNE] Iteration 250: error = 74.1814117, gradient norm = 0.1043922 (50 iterations in 2.781s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 74.181412\n",
      "[t-SNE] Iteration 300: error = 1.5108082, gradient norm = 0.0005867 (50 iterations in 3.001s)\n",
      "[t-SNE] Iteration 350: error = 1.4056307, gradient norm = 0.0002115 (50 iterations in 3.457s)\n",
      "[t-SNE] Iteration 400: error = 1.3533508, gradient norm = 0.0001293 (50 iterations in 3.607s)\n",
      "[t-SNE] Iteration 450: error = 1.2790587, gradient norm = 0.0003360 (50 iterations in 3.598s)\n",
      "[t-SNE] Iteration 500: error = 1.2567363, gradient norm = 0.0000805 (50 iterations in 3.481s)\n",
      "[t-SNE] Iteration 550: error = 1.2408911, gradient norm = 0.0000627 (50 iterations in 3.431s)\n",
      "[t-SNE] Iteration 600: error = 1.2185611, gradient norm = 0.0000572 (50 iterations in 3.501s)\n",
      "[t-SNE] Iteration 650: error = 1.1891152, gradient norm = 0.0001068 (50 iterations in 3.661s)\n",
      "[t-SNE] Iteration 700: error = 1.1801318, gradient norm = 0.0003782 (50 iterations in 3.527s)\n",
      "[t-SNE] Iteration 750: error = 1.1601160, gradient norm = 0.0000605 (50 iterations in 2.960s)\n",
      "[t-SNE] Iteration 800: error = 1.1301365, gradient norm = 0.0001148 (50 iterations in 2.638s)\n",
      "[t-SNE] Iteration 850: error = 1.1272136, gradient norm = 0.0000438 (50 iterations in 2.275s)\n",
      "[t-SNE] Iteration 900: error = 1.1117742, gradient norm = 0.0000515 (50 iterations in 2.292s)\n",
      "[t-SNE] Iteration 950: error = 1.1041161, gradient norm = 0.0000430 (50 iterations in 2.325s)\n",
      "[t-SNE] Iteration 1000: error = 1.0851470, gradient norm = 0.0000864 (50 iterations in 2.355s)\n",
      "[t-SNE] Iteration 1050: error = 1.0817493, gradient norm = 0.0000430 (50 iterations in 2.369s)\n",
      "[t-SNE] Iteration 1100: error = 1.0802453, gradient norm = 0.0000256 (50 iterations in 2.365s)\n",
      "[t-SNE] Iteration 1150: error = 1.0801741, gradient norm = 0.0000219 (50 iterations in 2.306s)\n",
      "[t-SNE] Iteration 1200: error = 1.0797738, gradient norm = 0.0000205 (50 iterations in 2.310s)\n",
      "[t-SNE] Iteration 1250: error = 1.0796432, gradient norm = 0.0000205 (50 iterations in 2.303s)\n",
      "[t-SNE] Iteration 1300: error = 1.0799671, gradient norm = 0.0000155 (50 iterations in 2.339s)\n",
      "[t-SNE] Iteration 1350: error = 1.0799881, gradient norm = 0.0000137 (50 iterations in 2.352s)\n",
      "[t-SNE] Iteration 1400: error = 1.0799825, gradient norm = 0.0000155 (50 iterations in 2.342s)\n",
      "[t-SNE] Iteration 1450: error = 1.0802932, gradient norm = 0.0000133 (50 iterations in 2.383s)\n",
      "[t-SNE] Iteration 1500: error = 1.0806650, gradient norm = 0.0000119 (50 iterations in 2.347s)\n",
      "[t-SNE] Iteration 1550: error = 1.0809317, gradient norm = 0.0000112 (50 iterations in 2.360s)\n",
      "[t-SNE] Iteration 1600: error = 1.0809007, gradient norm = 0.0000098 (50 iterations in 2.354s)\n",
      "[t-SNE] Iteration 1600: did not make any progress during the last 300 episodes. Finished.\n",
      "[t-SNE] KL divergence after 1600 iterations: 1.080901\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 696 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 696 samples in 0.068s...\n",
      "[t-SNE] Computed conditional probabilities for sample 696 / 696\n",
      "[t-SNE] Mean sigma: 2.154088\n",
      "[t-SNE] Computed conditional probabilities in 0.040s\n",
      "[t-SNE] Iteration 50: error = 80.9311371, gradient norm = 0.2379395 (50 iterations in 1.747s)\n",
      "[t-SNE] Iteration 100: error = 86.9918900, gradient norm = 0.2276230 (50 iterations in 1.751s)\n",
      "[t-SNE] Iteration 150: error = 91.7348251, gradient norm = 0.2142609 (50 iterations in 1.735s)\n",
      "[t-SNE] Iteration 200: error = 93.7457504, gradient norm = 0.2115575 (50 iterations in 1.750s)\n",
      "[t-SNE] Iteration 250: error = 95.1235046, gradient norm = 0.1959041 (50 iterations in 1.738s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 95.123505\n",
      "[t-SNE] Iteration 300: error = 2.7414293, gradient norm = 0.0008360 (50 iterations in 1.712s)\n",
      "[t-SNE] Iteration 350: error = 2.4005663, gradient norm = 0.0002979 (50 iterations in 1.906s)\n",
      "[t-SNE] Iteration 400: error = 2.2023010, gradient norm = 0.0001621 (50 iterations in 1.910s)\n",
      "[t-SNE] Iteration 450: error = 2.0754788, gradient norm = 0.0000996 (50 iterations in 1.932s)\n",
      "[t-SNE] Iteration 500: error = 1.9719357, gradient norm = 0.0000835 (50 iterations in 1.954s)\n",
      "[t-SNE] Iteration 550: error = 1.8798425, gradient norm = 0.0000596 (50 iterations in 2.041s)\n",
      "[t-SNE] Iteration 600: error = 1.7981077, gradient norm = 0.0000527 (50 iterations in 2.031s)\n",
      "[t-SNE] Iteration 650: error = 1.7331755, gradient norm = 0.0000462 (50 iterations in 2.104s)\n",
      "[t-SNE] Iteration 700: error = 1.6834768, gradient norm = 0.0000379 (50 iterations in 2.324s)\n",
      "[t-SNE] Iteration 750: error = 1.6407185, gradient norm = 0.0000349 (50 iterations in 2.284s)\n",
      "[t-SNE] Iteration 800: error = 1.6043330, gradient norm = 0.0000348 (50 iterations in 2.275s)\n",
      "[t-SNE] Iteration 850: error = 1.5767684, gradient norm = 0.0000423 (50 iterations in 2.241s)\n",
      "[t-SNE] Iteration 900: error = 1.5525048, gradient norm = 0.0000267 (50 iterations in 2.191s)\n",
      "[t-SNE] Iteration 950: error = 1.5339785, gradient norm = 0.0000227 (50 iterations in 2.128s)\n",
      "[t-SNE] Iteration 1000: error = 1.5131500, gradient norm = 0.0000266 (50 iterations in 2.047s)\n",
      "[t-SNE] Iteration 1050: error = 1.4961476, gradient norm = 0.0000208 (50 iterations in 1.982s)\n",
      "[t-SNE] Iteration 1100: error = 1.4830072, gradient norm = 0.0000161 (50 iterations in 1.905s)\n",
      "[t-SNE] Iteration 1150: error = 1.4716544, gradient norm = 0.0000148 (50 iterations in 1.823s)\n",
      "[t-SNE] Iteration 1200: error = 1.4594383, gradient norm = 0.0000137 (50 iterations in 1.804s)\n",
      "[t-SNE] Iteration 1250: error = 1.4455088, gradient norm = 0.0000130 (50 iterations in 1.758s)\n",
      "[t-SNE] Iteration 1300: error = 1.4316319, gradient norm = 0.0000123 (50 iterations in 1.713s)\n",
      "[t-SNE] Iteration 1350: error = 1.4154664, gradient norm = 0.0000138 (50 iterations in 1.620s)\n",
      "[t-SNE] Iteration 1400: error = 1.3959204, gradient norm = 0.0000173 (50 iterations in 1.572s)\n",
      "[t-SNE] Iteration 1450: error = 1.3806040, gradient norm = 0.0000140 (50 iterations in 1.609s)\n",
      "[t-SNE] Iteration 1500: error = 1.3698362, gradient norm = 0.0000121 (50 iterations in 1.521s)\n",
      "[t-SNE] Iteration 1550: error = 1.3619757, gradient norm = 0.0000100 (50 iterations in 1.490s)\n",
      "[t-SNE] Iteration 1600: error = 1.3546705, gradient norm = 0.0000097 (50 iterations in 1.434s)\n",
      "[t-SNE] Iteration 1650: error = 1.3459963, gradient norm = 0.0000101 (50 iterations in 1.404s)\n",
      "[t-SNE] Iteration 1700: error = 1.3397964, gradient norm = 0.0000079 (50 iterations in 1.372s)\n",
      "[t-SNE] Iteration 1750: error = 1.3330519, gradient norm = 0.0000072 (50 iterations in 1.306s)\n",
      "[t-SNE] Iteration 1800: error = 1.3255099, gradient norm = 0.0000066 (50 iterations in 1.257s)\n",
      "[t-SNE] Iteration 1850: error = 1.3171022, gradient norm = 0.0000066 (50 iterations in 1.244s)\n",
      "[t-SNE] Iteration 1900: error = 1.3084066, gradient norm = 0.0000078 (50 iterations in 1.246s)\n",
      "[t-SNE] Iteration 1950: error = 1.2983761, gradient norm = 0.0000099 (50 iterations in 1.280s)\n",
      "[t-SNE] Iteration 2000: error = 1.2923357, gradient norm = 0.0000079 (50 iterations in 1.277s)\n",
      "[t-SNE] Iteration 2050: error = 1.2884965, gradient norm = 0.0000067 (50 iterations in 1.248s)\n",
      "[t-SNE] Iteration 2100: error = 1.2853774, gradient norm = 0.0000058 (50 iterations in 1.282s)\n",
      "[t-SNE] Iteration 2150: error = 1.2825574, gradient norm = 0.0000052 (50 iterations in 1.273s)\n",
      "[t-SNE] Iteration 2200: error = 1.2799594, gradient norm = 0.0000049 (50 iterations in 1.252s)\n",
      "[t-SNE] Iteration 2250: error = 1.2774616, gradient norm = 0.0000047 (50 iterations in 1.260s)\n",
      "[t-SNE] Iteration 2300: error = 1.2747718, gradient norm = 0.0000045 (50 iterations in 1.224s)\n",
      "[t-SNE] Iteration 2350: error = 1.2721494, gradient norm = 0.0000041 (50 iterations in 1.195s)\n",
      "[t-SNE] Iteration 2400: error = 1.2698885, gradient norm = 0.0000039 (50 iterations in 1.202s)\n",
      "[t-SNE] Iteration 2450: error = 1.2678678, gradient norm = 0.0000037 (50 iterations in 1.199s)\n",
      "[t-SNE] Iteration 2500: error = 1.2659519, gradient norm = 0.0000035 (50 iterations in 1.186s)\n",
      "[t-SNE] Iteration 2550: error = 1.2642553, gradient norm = 0.0000034 (50 iterations in 1.200s)\n",
      "[t-SNE] Iteration 2600: error = 1.2626780, gradient norm = 0.0000033 (50 iterations in 1.190s)\n",
      "[t-SNE] Iteration 2650: error = 1.2611728, gradient norm = 0.0000032 (50 iterations in 1.224s)\n",
      "[t-SNE] Iteration 2700: error = 1.2592763, gradient norm = 0.0000031 (50 iterations in 1.209s)\n",
      "[t-SNE] Iteration 2750: error = 1.2571634, gradient norm = 0.0000031 (50 iterations in 1.216s)\n",
      "[t-SNE] Iteration 2800: error = 1.2547376, gradient norm = 0.0000032 (50 iterations in 1.177s)\n",
      "[t-SNE] Iteration 2850: error = 1.2520970, gradient norm = 0.0000029 (50 iterations in 1.234s)\n",
      "[t-SNE] Iteration 2900: error = 1.2502178, gradient norm = 0.0000029 (50 iterations in 1.228s)\n",
      "[t-SNE] Iteration 2950: error = 1.2480944, gradient norm = 0.0000030 (50 iterations in 1.226s)\n",
      "[t-SNE] Iteration 3000: error = 1.2453378, gradient norm = 0.0000031 (50 iterations in 1.209s)\n",
      "[t-SNE] Iteration 3050: error = 1.2429364, gradient norm = 0.0000029 (50 iterations in 1.240s)\n",
      "[t-SNE] Iteration 3100: error = 1.2404118, gradient norm = 0.0000028 (50 iterations in 1.230s)\n",
      "[t-SNE] Iteration 3150: error = 1.2382443, gradient norm = 0.0000027 (50 iterations in 1.199s)\n",
      "[t-SNE] Iteration 3200: error = 1.2365026, gradient norm = 0.0000026 (50 iterations in 1.187s)\n",
      "[t-SNE] Iteration 3250: error = 1.2348682, gradient norm = 0.0000025 (50 iterations in 1.228s)\n",
      "[t-SNE] Iteration 3300: error = 1.2332666, gradient norm = 0.0000025 (50 iterations in 1.217s)\n",
      "[t-SNE] Iteration 3350: error = 1.2316713, gradient norm = 0.0000024 (50 iterations in 1.219s)\n",
      "[t-SNE] Iteration 3400: error = 1.2298595, gradient norm = 0.0000024 (50 iterations in 1.201s)\n",
      "[t-SNE] Iteration 3450: error = 1.2281765, gradient norm = 0.0000024 (50 iterations in 1.224s)\n",
      "[t-SNE] Iteration 3500: error = 1.2265353, gradient norm = 0.0000023 (50 iterations in 1.202s)\n",
      "[t-SNE] Iteration 3550: error = 1.2247965, gradient norm = 0.0000022 (50 iterations in 1.195s)\n",
      "[t-SNE] Iteration 3600: error = 1.2229367, gradient norm = 0.0000021 (50 iterations in 1.211s)\n",
      "[t-SNE] Iteration 3650: error = 1.2208802, gradient norm = 0.0000021 (50 iterations in 1.213s)\n",
      "[t-SNE] Iteration 3700: error = 1.2184834, gradient norm = 0.0000021 (50 iterations in 1.242s)\n",
      "[t-SNE] Iteration 3750: error = 1.2157580, gradient norm = 0.0000022 (50 iterations in 1.236s)\n",
      "[t-SNE] Iteration 3800: error = 1.2123985, gradient norm = 0.0000022 (50 iterations in 1.208s)\n",
      "[t-SNE] Iteration 3850: error = 1.2088828, gradient norm = 0.0000023 (50 iterations in 1.204s)\n",
      "[t-SNE] Iteration 3900: error = 1.2050925, gradient norm = 0.0000022 (50 iterations in 1.211s)\n",
      "[t-SNE] Iteration 3950: error = 1.2020231, gradient norm = 0.0000021 (50 iterations in 1.217s)\n",
      "[t-SNE] Iteration 4000: error = 1.1989350, gradient norm = 0.0000022 (50 iterations in 1.205s)\n",
      "[t-SNE] Iteration 4050: error = 1.1959628, gradient norm = 0.0000021 (50 iterations in 1.180s)\n",
      "[t-SNE] Iteration 4100: error = 1.1936890, gradient norm = 0.0000020 (50 iterations in 1.188s)\n",
      "[t-SNE] Iteration 4150: error = 1.1918322, gradient norm = 0.0000019 (50 iterations in 1.161s)\n",
      "[t-SNE] Iteration 4200: error = 1.1903125, gradient norm = 0.0000019 (50 iterations in 1.138s)\n",
      "[t-SNE] Iteration 4250: error = 1.1889381, gradient norm = 0.0000018 (50 iterations in 1.210s)\n",
      "[t-SNE] Iteration 4300: error = 1.1878256, gradient norm = 0.0000018 (50 iterations in 1.183s)\n",
      "[t-SNE] Iteration 4350: error = 1.1868923, gradient norm = 0.0000018 (50 iterations in 1.188s)\n",
      "[t-SNE] Iteration 4400: error = 1.1859412, gradient norm = 0.0000016 (50 iterations in 1.189s)\n",
      "[t-SNE] Iteration 4450: error = 1.1852427, gradient norm = 0.0000016 (50 iterations in 1.212s)\n",
      "[t-SNE] Iteration 4500: error = 1.1846424, gradient norm = 0.0000015 (50 iterations in 1.188s)\n",
      "[t-SNE] Iteration 4550: error = 1.1840155, gradient norm = 0.0000015 (50 iterations in 1.210s)\n",
      "[t-SNE] Iteration 4600: error = 1.1834803, gradient norm = 0.0000014 (50 iterations in 1.204s)\n",
      "[t-SNE] Iteration 4650: error = 1.1828612, gradient norm = 0.0000014 (50 iterations in 1.202s)\n",
      "[t-SNE] Iteration 4700: error = 1.1824688, gradient norm = 0.0000014 (50 iterations in 1.213s)\n",
      "[t-SNE] Iteration 4750: error = 1.1820208, gradient norm = 0.0000013 (50 iterations in 1.194s)\n",
      "[t-SNE] Iteration 4800: error = 1.1816880, gradient norm = 0.0000013 (50 iterations in 1.190s)\n",
      "[t-SNE] Iteration 4850: error = 1.1812661, gradient norm = 0.0000013 (50 iterations in 1.217s)\n",
      "[t-SNE] Iteration 4900: error = 1.1809386, gradient norm = 0.0000013 (50 iterations in 1.227s)\n",
      "[t-SNE] Iteration 4950: error = 1.1804271, gradient norm = 0.0000013 (50 iterations in 1.187s)\n",
      "[t-SNE] Iteration 5000: error = 1.1799241, gradient norm = 0.0000013 (50 iterations in 1.229s)\n",
      "[t-SNE] Iteration 5050: error = 1.1791995, gradient norm = 0.0000013 (50 iterations in 1.209s)\n",
      "[t-SNE] Iteration 5100: error = 1.1785842, gradient norm = 0.0000013 (50 iterations in 1.226s)\n",
      "[t-SNE] Iteration 5150: error = 1.1779866, gradient norm = 0.0000013 (50 iterations in 1.210s)\n",
      "[t-SNE] Iteration 5200: error = 1.1774153, gradient norm = 0.0000012 (50 iterations in 1.223s)\n",
      "[t-SNE] Iteration 5250: error = 1.1769625, gradient norm = 0.0000012 (50 iterations in 1.194s)\n",
      "[t-SNE] Iteration 5300: error = 1.1765312, gradient norm = 0.0000012 (50 iterations in 1.222s)\n",
      "[t-SNE] Iteration 5350: error = 1.1759956, gradient norm = 0.0000012 (50 iterations in 1.237s)\n",
      "[t-SNE] Iteration 5400: error = 1.1755896, gradient norm = 0.0000012 (50 iterations in 1.192s)\n",
      "[t-SNE] Iteration 5450: error = 1.1751981, gradient norm = 0.0000011 (50 iterations in 1.205s)\n",
      "[t-SNE] Iteration 5500: error = 1.1748058, gradient norm = 0.0000011 (50 iterations in 1.204s)\n",
      "[t-SNE] Iteration 5550: error = 1.1743774, gradient norm = 0.0000011 (50 iterations in 1.245s)\n",
      "[t-SNE] Iteration 5600: error = 1.1738890, gradient norm = 0.0000011 (50 iterations in 1.239s)\n",
      "[t-SNE] Iteration 5650: error = 1.1734263, gradient norm = 0.0000011 (50 iterations in 1.222s)\n",
      "[t-SNE] Iteration 5700: error = 1.1727644, gradient norm = 0.0000012 (50 iterations in 1.200s)\n",
      "[t-SNE] Iteration 5750: error = 1.1720906, gradient norm = 0.0000013 (50 iterations in 1.192s)\n",
      "[t-SNE] Iteration 5800: error = 1.1709887, gradient norm = 0.0000013 (50 iterations in 1.206s)\n",
      "[t-SNE] Iteration 5850: error = 1.1699516, gradient norm = 0.0000012 (50 iterations in 1.208s)\n"
     ]
    }
   ],
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=10000, verbose=3).fit_transform(element_data_train_scaled)\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=10000, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=10000, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=10000, verbose=3).fit_transform(element_data_test_scaled)\n",
    "my_tsne_everything = TSNE(n_components=3, n_iter=10000, verbose=3).fit_transform(element_data_everything_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the t-SNE dimensions for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_everything = pd.DataFrame(data = my_tsne_everything, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_data = train_data[train_data['Geology'] == 'Bedrock']\n",
    "superficial_data = train_data[train_data['Geology'] == 'Superficial'].reset_index(drop = True)\n",
    "artefact_data = pd.concat([test_data.reset_index(drop = True), X_test_labeled_df['inlierLabel'].reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addartefactclass(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['class'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['class'])\n",
    "    else:\n",
    "        return('Artefact')\n",
    "my_data['class'] = my_data.swifter.apply(addartefactclass, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artefact_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_mod = my_data[my_data['Geology'] != 'Artefacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_mod_final = pd.concat([my_data_mod, artefact_data], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inlier_label(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['class'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['class'])\n",
    "    else:\n",
    "        return(row['inlierLabel'])\n",
    "my_data_mod_final['class'] = my_data_mod_final.swifter.apply(inlier_label, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_mod_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train_labelled = pd.concat([tsne_df_train.reset_index(drop = True), train_data['class'].reset_index(drop = True)], axis = 1)\n",
    "tsne_df_bedrock_train_labelled = pd.concat([tsne_df_bedrock_train.reset_index(drop = True), bedrock_data['class'].reset_index(drop = True)], axis = 1)\n",
    "tsne_df_superficial_train_labelled = pd.concat([tsne_df_superficial_train, superficial_data['class']], axis = 1)\n",
    "tsne_df_test['class'] = 1\n",
    "tsne_df_everything_labelled = pd.concat([tsne_df_everything.reset_index(drop = True), my_data_mod_final['class'].reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train_labelled = pd.concat([PC_df_train.reset_index(drop = True), train_data['class'].reset_index(drop = True)], axis = 1)\n",
    "PC_df_superficial_train_labelled = pd.concat([PC_df_superficial_train.reset_index(drop = True), superficial_data['class']], axis = 1)\n",
    "PC_df_bedrock_train_labelled = pd.concat([PC_df_bedrock_train.reset_index(drop = True), bedrock_data['class'].reset_index(drop = True)], axis = 1)\n",
    "PC_df_everything_labelled = pd.concat([PC_df_everything.reset_index(drop = True), my_data_mod_final['class'].reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_everything_labelled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne_df_train_labelled.to_csv('website/tsne_both.csv')\n",
    "#tsne_df_bedrock_train_labelled.to_csv('website/tsne_bedrock.csv')\n",
    "#tsne_df_superficial_train_labelled.to_csv('website/tsne_superficial.csv')\n",
    "#tsne_df_test.to_csv('website/tsne_artefacts.csv')\n",
    "#tsne_df_everything_labelled.to_csv('website/tsne_samples_and_artefacts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PC_df_train_labelled.to_csv('website/pca_both.csv')\n",
    "#PC_df_superficial_train_labelled.to_csv('website/pca_superficial.csv')\n",
    "#PC_df_bedrock_train_labelled.to_csv('website/pca_bedrock.csv')\n",
    "#PC_df_everything_labelled.to_csv('website/pca_samples_and_artefacts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
